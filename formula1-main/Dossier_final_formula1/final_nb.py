# -*- coding: utf-8 -*-
"""final_nb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KfGjVzQgD9PSH6xV1STHZyaijbeUkoI-

#I. Data import and discovery

### Installing the packages we used
"""

pip install requests

pip install lxml

pip install selenium

pip install geopandas

"""* requests is a package used to retrieve data from a website
* lxml is a package for downloading lxml files
* selenium is a package for automated interaction with a server

## Importing the libraries we've used
"""

import lxml
import selenium
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import requests
import bs4
import geopandas as gpd
from shapely.geometry import Point
from geopandas import GeoDataFrame
from sklearn.metrics import confusion_matrix, precision_score
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn import svm
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.neural_network import MLPClassifier, MLPRegressor
import matplotlib.pyplot as plt
sns.set(rc={'figure.figsize':(20,10)})
sns.set_theme()

"""## Importing data from an API

** Data retrieval from an online API: "ergast.com".
* For ease of reading, we've imported the data into another notebook and saved it as a csv. Here we import the data directly from the csv.

### Base "races"

*Objective: creation of a "Races" database containing the years of the seasons, the name and rank of the circuit in the season, the latitude and longitude of the circuit (useful later for geographical representations), the country and date of the circuit, and the associated wikipedia url.
* The database contains 550 lines corresponding to seasons between 1990 and 2023.
"""

races=pd.read_csv('race.csv', index_col=0)

races.head()

races.info()

"""## Database "Results"

Objective: creation of a second database including starting grid position and final podium.
* Observations of missing values on the "time" variable: we therefore chose to exclude it (64% missing values).
"""

results=pd.read_csv('results.csv', index_col=0)

results.head()

results.info()

results_na=results.copy()
sns.heatmap(results_na.isna())
results_na.isna()['time'].mean()

results=results.drop(labels='time', axis=1)

"""###  DataBase "driver_standings"

* Objective: create a database containing drivers' win and points histories for the season
"""

driver_standings=pd.read_csv('driver_standings.csv', index_col=0)

driver_standings.info()

driver_standings.head()

"""#### Creation of the "constructor_standings" database

* Objective: to create a database containing the history of victories and points over the season for manufacturers
"""

constructor_standings=pd.read_csv('constructor_standings.csv', index_col=0)

constructor_standings.info()

constructor_standings.head()

"""### Creation of the "qualifying_results" database

** Objective: to create a database containing information on qualifiers, such as time and position.
"""

qualifying_results=pd.read_csv('qualifying_results.csv', index_col=0)

qualifying_results.info()

qualifying_results.head()

"""###  Database "weather"

Objective: create a second database including weather conditions for each race
"""

weather=pd.read_csv('weather.csv', index_col=0)

weather.info()

weather.head()

"""#II- Descriptive statistics and study of the link between our variables and victory

In this section, we'll be looking at the link between the variables available to us and our target, i.e. final victory. The idea is to estimate whether the "intuitive" explanatory variables, such as the starting grid, the manufacturer or the weather, hold any signal for predicting victory.

### II-1. Correlation between Start Grid and Finish Ranking

* La première idée est d'étudier le lien entre la grille de départ et le classement final. En effet, cela nous paraît le premier critère qui va determiner l'issue de la course

## First correlation study

* We plot the point cloud for all the data available to us
"""

sns.relplot(x="grid", y="podium", data=results)

"""* Comments: At first glance, there are no clear correlations between initial grid position and finish position. This is probably due to the sheer volume of data and the totally different years we're studying: there's a lot of noise.

* We then plot this point cloud for the year 2020, to give a better visualization of the points, and by circuit, to visualize trends by circuit.
"""

results_2020=results.copy()
results_2020=results_2020[results_2020["season"]>=2020]
sns.relplot(x="grid", y="podium", data=results_2020[results_2020["season"]==2020],hue="circuit_id")

"""* Comments: No clear correlation yet, but a linear trend is emerging.

### Creation of a correlation function between the grid and the podium
"""

results_2010=results[results["season"]>=2010].copy()

"""* This function gives the correlation between the podium and the grid for each circuit in the chosen year."""

def corr_grid_podium(annee):

    circuits=results_2010[results_2010["season"] ==annee]["circuit_id"].unique()
    df=results_2010[results_2010["season"] ==annee]
    les_correlations={}

    for circuit in circuits:
        df1=df[df["circuit_id"]==circuit][['grid',"podium"]]
        corr=df1.corr()["podium"][0]
        les_correlations[circuit]=corr

    df2=pd.DataFrame(list(les_correlations.items()),
                   columns=['circuit_id', 'correlation_grid_pod'])
    df2=df2.sort_values(by='correlation_grid_pod',ascending=False)

    print(df2)

    _,(ax1) = plt.subplots(ncols=1)
    sns.barplot(data=df2, x='circuit_id', y='correlation_grid_pod', ax=ax1, palette=sns.color_palette("icefire"))

print(corr_grid_podium(2019)) # We chose 2019

"""* Observations: A correlation close to 0.84 (close to 1) is observed for the Monaco circuit, which is consistent with the fact that the streets are very narrow for overtaking. In contrast, it's very easy to manoeuvre on the SPa circuit, hence a much lower correlation.

# II-2. Correlation between manufacturer and grid and finish rankings

* Our second idea is to study the importance of the builder in the final victory.

## Average position of manufacturers on the grid and at the finish
"""

results_constructeur=results.copy()
results_constructeur=results_constructeur[["constructor","podium","grid"]]

results_constructeur.groupby("constructor").mean().sort_values(by="podium",ascending=True)

"""* Observations: As expected, Mercedes, Ferrari and RedBull have the best average starting positions and finishing positions.

## Percentage of races won by manufacturer since 1990
"""

results_constructeur_1=results_constructeur.copy()
results_constructeur_1=results_constructeur_1[results_constructeur_1["podium"]==1]
df=(results_constructeur_1.groupby("constructor").count())/len(results_constructeur_1)*100
df=df.reset_index()
_,(ax1) = plt.subplots(ncols=1)
sns.barplot(data=df, x='constructor', y='podium', ax=ax1, palette=sns.color_palette("icefire"))

"""* Observations: here again, without much surprise, some manufacturers stand out from the crowd, such as Ferrari, McLaren and Mercedes. There's a huge disparity between the manufacturers, so each one contains a signal that can be used to predict the final victory.

# II-3. Correlation between driver nationality and final victory

* We then turn to the link between nationality and victory. We're going to find
out whether this variable can explain or predict final victory. We'll simply
plot the percentage of victories by nationality.
"""

results_nationality=results.copy()
results_nationality=results_nationality[["nationality","grid","podium","circuit_id"]]

nb_courses=len(results_nationality[results_nationality["podium"]==1])
df=((results_nationality[results_nationality["podium"]==1].groupby("nationality").count())/(nb_courses))*100
df=df.reset_index()
sns.barplot(x='nationality',y='podium',data=df,palette=sns.color_palette("icefire"))

"""* Comments: Unsurprisingly, the drivers with the most Grand Prix titles are German (Vettel, Schumacher, Rosberg), English (Hamilton), Brazilian (Senna) and Finnish (Räikkönen). There is a disparity between nationalities, which means that nationality is an important factor."""

# Add nationality
df_country_race=races[["circuit_id","country"]].copy()
df_country_race.head()
results_merged=results.merge(df_country_race ,how='left', on="circuit_id")

print(results_merged)
print(results_merged["nationality"].unique())
print(results_merged["country"].unique())

"""## II-4. Correlation between age and grand prix wins

* We now turn to the link between age and ultimate victory.

* To do this, we create the variable age in years below, which corresponds to the age at the time of the grand prix.
"""

results_age=results.copy()

results_age["date_of_birth"]=pd.to_datetime(results_age["date_of_birth"])

results_age["season"]=pd.to_datetime(results_age["season"], format="%Y") # conversion - format date

results_age["age"]=round(((results_age["season"]-results_age['date_of_birth']).dt.days)/365)

"""* Visualize created data"""

results_age["date_of_birth"]

results_age["season"]

results_age["age"]

"""### Correlation between the podium and the age of the drivers at the time of the race

> Indented block


"""

df=results_age[["age","podium"]]

corr = df.corr()
sns.heatmap(corr,annot=True,vmin=0, vmax=1)

"""* Comments: There's a slight negative correlation between age and ranking: in other words, the older you are, the lower your ranking and the closer you are to victory.

### Victories by age group

* The number of victories per age is plotted, along with an approximation of

---

the density.
"""

df_gagnant=df[df["podium"]==1]
sns.histplot(data=df_gagnant, x='age',kde=True)

"""* Comments: There's a peak at 30 years of age, which corresponds to the right ratio between age and experience. Once again, we see a disparity in the distribution, which means that age contains information that can be used to predict victory.

## II-6. Weather analysis
"""

weather_merged=pd.merge(results, weather, how="right", left_on="circuit_id", right_on="circuit_id")

weather_merged["weather"].nunique()

"""* There are 363 different types of tense, so we're going to look at the most common ones.

First, we look at dry circuits
"""

results_2010=weather_merged.copy()
results_2010=results_2010[results_2010["season_x"]>=2010]
circuits=results_2010[results_2010["season_x"] ==2010]["circuit_id"].unique()
df=results_2010[results_2010["season_x"] ==2010]
df_dry=df[df["weather_dry"]==1]
les_correlations={}

for circuit in circuits:
    df_dry_1=df_dry[df_dry["circuit_id"]==circuit][['grid',"podium"]]
    corr=df_dry_1.corr()["podium"][0]
    les_correlations[circuit]=corr

df_dry_2=pd.DataFrame(list(les_correlations.items()),
                   columns=['circuit_id', 'correlation_grid_pod'])
df_dry_2=df_dry_2.sort_values(by='correlation_grid_pod',ascending=False)

print(df_dry_2)

_,(ax1) = plt.subplots(ncols=1)
sns.barplot(data=df_dry_2, x='circuit_id', y='correlation_grid_pod', ax=ax1, palette=sns.color_palette("icefire"))

"""* Comments: On circuits such as Bahrain and the Hockenheimring, there's a strong correlation between grid position and final podium finish. On these dry circuits, a good grid position virtually guarantees a final podium finish, and this correlation is stronger than when climate is not taken into account.

Next, we look at wet circuits
"""

circuits=results_2010[results_2010["season_x"] ==2010]["circuit_id"].unique()
df=results_2010[results_2010["season_x"] ==2010]
df_wet=df[df["weather_wet"]==1]
les_correlations={}

for circuit in circuits:
    df_wet_1=df_wet[df_wet["circuit_id"]==circuit][['grid',"podium"]]
    corr=df_wet_1.corr()["podium"][0]
    les_correlations[circuit]=corr

df_wet_2=pd.DataFrame(list(les_correlations.items()),
                   columns=['circuit_id', 'correlation_grid_pod'])
df_wet_2=df_wet_2.sort_values(by='correlation_grid_pod',ascending=False)

print(df_wet_2)

_,(ax1) = plt.subplots(ncols=1)
sns.barplot(data=df_wet_2, x='circuit_id', y='correlation_grid_pod', ax=ax1, palette=sns.color_palette("icefire"))

"""* Comments: Once again, there's a strong correlation between grid position and final podium finish at the Hockenheimring, meaning that a good grid position virtually guarantees a final podium finish, with this correlation being stronger than when climate is not taken into account. This reinforces the idea that the Hockenheimring circuit leaves little room for suspense.

## II-7. Viewing the various circuits on the board
"""

cities=pd.DataFrame({'circuit_id':races['circuit_id'],
                    'lat':races['lat'],
                    'long':races['long']})

geometry = [Point(xy) for xy in zip(races['long'], races['lat'])]
gdf = GeoDataFrame(races, geometry=geometry)

world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
gdf.plot(ax=world.plot(figsize=(15, 7)), marker='o', color='red', markersize=10);

"""# III- Modeling

## III-1. Preparing data for modeling

The idea is to create a single dataframe in which we can find all the variables a priori useful for modeling, as well as our target, i.e. the race winner (or a proxy: the race ranking). To do this, we'll need to merge the dataframes, manage missing values and prepare the categorical variables.

### Preparing the dataframe merge
"""

qualifying_results.info()

qualifying_results["driver_name"]=qualifying_results["driver_name"].str.split()
index=qualifying_results.columns.get_loc("driver_name")
taille=len(qualifying_results)

for i in range(taille):

    qualifying_results.iloc[i,index]=qualifying_results.iloc[i,index][1]

qualifying_results.rename(columns={'driver_name': 'driver'}, inplace=True)
qualifying_results["driver"]=qualifying_results["driver"].str.lower()

"""### Merge dataframes

Nous allons merger les dataframes à partir du dataframe results car ce dernier est selon nous le plus complet. Nous allons ainsi merger les dataframes les uns après les autres sur des clés uniques et communes avec la méthode "left"
"""

results_2=results.merge(how="left",on=["season","round","constructor"],right=constructor_standings)
results_2.head()
results_3=results_2.merge(how="left",on=["season","round","driver"],right=driver_standings)
results_3.head(30)
results_4=results_3.merge(how="left",on=["season","round","driver"],right=qualifying_results)
results_5=results_4.merge(how="left",on=["season","round","circuit_id"],right=weather)

# Nous renommons quelques variables et nous conservons notre dataframe mergé sous le nom de "df_merged"

results_5=results_5.drop(columns="grid_y")
results_5.rename(columns={'grid_x': 'grid'}, inplace=True)
df_merged=results_5.copy()

"""We now have a dataframe containing all our metrics: df_merged:"""

df_merged.info()

# Visualisation rapide des valeurs manquantes par variable

df_merged_na=df_merged.isna().copy()
sns.heatmap(df_merged_na)

"""* We'll also have to manage a few missing values for 'qualifying_time', 'driver_wins', 'driver_points', driver_standings_pos in particular.

### Managing missing values

* We replace some missing values with 0s (see below for important categories), which is very simple to implement and plausible for these missing values.
* We then delete all lines still containing missing values.
"""

x_var= ['driver_points', 'driver_wins', 'driver_standings_pos', 'constructor_points',
            'constructor_wins' , 'constructor_standings_pos']
for var in x_var:
    df_merged[var].fillna(0, inplace = True)
    df_merged[var] = df_merged[var].map(lambda x: int(x))

df_merged.dropna(inplace = True )

"""### Creating new variables

We're now going to create some variables which are not directly accessible in our data, but which will undoubtedly enable us to predict the winner of the race. We'll create the "age" variable, which corresponds to the age (to the nearest year) of the drivers, and the "qualifying_time" variable, which contains the cumulative difference between the qualifying times of the drivers on pole position.
"""

df_merged["date_of_birth"]=pd.to_datetime(df_merged["date_of_birth"])
df_merged["season"]=pd.to_datetime(df_merged["season"], format="%Y") # conversion - format date
df_merged["age"]=round(((df_merged["season"]-df_merged['date_of_birth']).dt.days)/365)

df_merged['qualifying_time'] = df_merged.qualifying_time.map(lambda x: 0 if str(x) == '00.000' else(float(str(x).split(':')[1]) + (60 * float(str(x).split(':')[0])) if len(str(x).split(':'))>1 else 0))
df_merged = df_merged[df_merged['qualifying_time'] != 0]
df_merged.sort_values(['season', 'round', 'grid'], inplace = True)
df_merged['qualifying_time_diff'] = df_merged.groupby(['season', 'round']).qualifying_time.diff()
df_merged['qualifying_time'] = df_merged.groupby(['season', 'round']).qualifying_time_diff.cumsum().fillna(0)
df_merged.drop('qualifying_time_diff', axis = 1, inplace = True)

df_merged['qualifying_time']

"""### Preparing our categorical variables for modeling

We have multiple categorical variables, such as manufacturer, nationality and circuit variables. These variables are a priori useful for predicting the winner of a grand prix. So, to be able to use them in our modeling, we need to represent them in the form of an indicator:
"""

df_ind = pd.get_dummies(df_merged, columns = ['circuit_id', 'nationality', 'constructor'] )

for col in df_ind.columns:
    if 'nationality' in col and df_ind[col].sum() < 100:
        df_ind.drop(columns=col,inplace=True)

    elif 'constructor' in col and df_ind[col].sum() < 100:
        df_ind.drop(columns=col,inplace=True)

    elif 'circuit_id' in col and df_ind[col].sum() < 100:
        df_ind.drop(columns=col,inplace=True)

del df_ind['date_of_birth']
del df_ind['car']
del df_ind['weather']

df_ind['season']=pd.to_datetime(df_ind['season'])
df_ind['season'] = df_ind['season'].dt.year

final_merged=df_ind.copy()

np.set_printoptions(precision=4)

"""## III-2. Classification

* With this project, we want to predict the winner of an F1 grand prix. We can then proceed by classification, considering that the winner is in category 1 and the loser in category 0.

* To evaluate the performance of our model, we decided to train our model on the years prior to 2020 and measure the performance of our model as the percentage of races "well" predicted in 2020. There are other approaches, but we chose the latter because it allows us to intuitively assess the relevance or otherwise of the model over a recent year.
"""

comparison_dict ={'model':[],
                  'params': [],
                  'score': []}

df = final_merged.copy()

df["podium"] = df["podium"].map(lambda x: 1 if x == 1 else 0) #(losers vs winners)

train = df[df["season"]<2020]
X_train = train.drop(['driver', 'podium','points'], axis = 1)
y_train = train["podium"]

scaler = StandardScaler()
X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)

"""* We implement a function that returns the performance of our model, i.e. the percentage of "good" races predicted for 2020."""

def score_classification(model):
    score = 0
    for circuit in df[df.season == 2020]['round'].unique():

        test = df[(df.season == 2020) & (df['round'] == circuit)]
        X_test = test.drop(columns=["driver","podium","points"])
        y_test = test['podium']

        #scaling
        X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)

        # make predictions
        prediction_df = pd.DataFrame(model.predict_proba(X_test), columns = ['proba_0', 'proba_1'])
        prediction_df['actual'] = y_test.reset_index(drop = True)
        prediction_df.sort_values('proba_1', ascending = False, inplace = True)
        prediction_df.reset_index(inplace = True, drop = True)
        prediction_df['predicted'] = prediction_df.index
        prediction_df['predicted'] = prediction_df['predicted'].map(lambda x: 1 if x == 0 else 0)

        score += precision_score(prediction_df.actual, prediction_df.predicted)

    model_score = score / df[df.season == 2020]['round'].unique().max()
    return model_score

"""###  SVM Classifier

* We've chosen to use the SVM model for classification, as it's a very common way of solving classification problems.

* To find the model parameters that give the best score, we'll test different parameter combinations. This code is fairly (if not very) slow to run.
"""

params={'gamma': np.logspace(-4, -1, 20),
        'C': np.logspace(-2, 1, 20),
        'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}

for gamma in params['gamma']:
    for c in params['C']:
        for kernel in params['kernel']:
            model_params = (gamma, c, kernel)
            model = svm.SVC(probability = True, gamma = gamma, C = c, kernel = kernel )
            model.fit(X_train, y_train)

            model_score = score_classification(model)

            comparison_dict['model'].append('svm_classifier')
            comparison_dict['params'].append(model_params)
            comparison_dict['score'].append(model_score)

"""## III-3. Regression

* We can also consider our problem as a regression problem. Indeed, race ranking can be seen as a continuous variable, so the winner will be the one with the lowest ranking prediction.

* In order to compare our classification with our regression, we use the same test data (2020) and the same model performance indicator.
"""

df = final_merged.copy()

train = df[df.season<2020]
X_train = train.drop(['driver', 'podium','points'], axis = 1)
y_train = train.podium

scaler = StandardScaler()
X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)

"""* We implement a function that returns the performance of our model, i.e. the percentage of "good" races predicted for 2020."""

def score_regression(model,xvar):
    score=0
    les_circuits=df[df['season']==2020]["round"].unique()
    for circuit in les_circuits:
        test=df[(df['season']==2020) & (df['round']==circuit)]
        X_test=test[xvar]
        y_test=test["podium"]

        X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)


        prediction_df = pd.DataFrame(model.predict(X_test), columns = ['results'])
        prediction_df['podium'] = y_test.reset_index(drop = True)
        prediction_df['actual'] = prediction_df['podium'].map(lambda x: 1 if x == 1 else 0)
        prediction_df.sort_values('results', ascending = True, inplace = True)
        prediction_df.reset_index(inplace = True, drop = True)
        prediction_df['predicted'] = prediction_df.index
        prediction_df['predicted'] = prediction_df.predicted.map(lambda x: 1 if x == 0 else 0)

        score += precision_score(prediction_df['actual'],prediction_df['predicted'])

    model_score = score / df[df.season == 2020]['round'].unique().max()
    return model_score

"""### Linear regression

* For regression, we chose to use linear regression. This is a very standard model, easy to interpret and relatively quick to run.

* First, we'll regress on all the explanatory variables to store their scores in our comparison dictionary.

* In a second step, we'll regress on different combinations of variables to understand which types of variables best predict the race winner.
"""

model = LinearRegression(fit_intercept = 'True')
model.fit(X_train, y_train)
model_score=score_regression(model,train.columns.drop(["driver","podium",'points']))
comparison_dict['model'].append('regression_lineaire')
comparison_dict['params'].append('True')
comparison_dict['score'].append(model_score)

"""* We now regress on different combinations of variable groups:
- weather for all weather data
- grid for all qualifying data
- circuit for all circuit-related data
- constructor for all constructor data

"""

xvar_total=df.columns.drop(["driver",'podium'])
xvar_series=pd.Series(xvar_total)

xvar_weather=list(xvar_series[xvar_series.str.contains("weather")])
xvar_nationality=list(xvar_series[xvar_series.str.contains("nationality")])
xvar_circuit=list(xvar_series[xvar_series.str.contains("circuit")])
xvar_constructor=list(xvar_series[xvar_series.str.contains("constructor")])
xvar_grid=list(["grid","constructor_standings_pos","driver_standings_pos","qualifying_time"])

xvar_comparison={'xvar':[],'score':[]}

xvar_combinaison=[xvar_weather+xvar_grid,xvar_grid+xvar_circuit,xvar_nationality+xvar_circuit,xvar_constructor+xvar_circuit,xvar_weather+xvar_grid+xvar_constructor]
xvar_comparison['type_var']=["weather+grid","grid+circuit",'nationality+circuit','constructor+circuit',"weather+grid+constructor"]

for xvar in xvar_combinaison:
    X_train_2 = train[xvar].copy()
    y_train_2 = train["podium"].copy()

    scaler = StandardScaler()
    X_train_2 = pd.DataFrame(scaler.fit_transform(X_train_2), columns = X_train_2.columns)

    model = LinearRegression(fit_intercept = 'True')
    model.fit(X_train_2, y_train_2)

    model_score=score_regression(model,xvar)


    xvar_comparison['xvar'].append(xvar)
    xvar_comparison['score'].append(model_score)

comparaison_xvar=pd.DataFrame(xvar_comparison)
comparaison_xvar=comparaison_xvar.groupby("type_var")["score"].max().reset_index()
print(comparaison_xvar)
_,(ax1) = plt.subplots(ncols=1)
sns.barplot(data=comparaison_xvar, x='type_var', y='score', ax=ax1, palette=sns.color_palette("icefire"))

"""* Comments: Some combinations of variables are better at predicting the final result. For example, the constructor+circuit combination scores 52%, while the nationality+circuit combination scores 5%.

# III-4. Comparison of our two models

* In this final modeling section, we'll compare our models by comparing the maximum scores obtained.
"""

comparaison_model=pd.DataFrame(comparison_dict)
comparaison_model=comparaison_model.groupby("model")["score"].max().reset_index()
print(comparaison_model)
_,(ax1) = plt.subplots(ncols=1)
sns.barplot(data=comparaison_model, x='model', y='score', ax=ax1, palette=sns.color_palette("icefire"))

"""* We note that linear regression scores 65% versus 70% for the SVM classifier.
* However, we note that linear regression runs much faster than the SVM classifier.
"""